#!/usr/bin/env python
# coding: utf-8

# # Data Cleaning with Python

# In this project, I discuss various useful techniques to clean a dataset with Python, NumPy and Pandas.
# 
# This project is divided into various sections. Each section does specific task.

# ## Table of Contents:-

# 1. Introduction to Python data cleaning  
# &nbsp;  
# 2. Python data cleansing – prerequisites  
# &nbsp;  
# 3. Import the required Python libraries  
# &nbsp;  
# 4. The source dataset  
# &nbsp;  
# 5. Exploratory data analysis (EDA)  
# &nbsp;  
# 6. Visual exploratory data analysis (Visual EDA)  
# &nbsp;  
# 7. Findings of EDA and Visual EDA  
# &nbsp;  
# 8. Split the ‘age_sex’ column into two separate columns  
# &nbsp;  
# 9. Reorder the column labels  
# &nbsp;  
# 10. Dealing with negative numerical values  
# &nbsp;  
# 11. Dealing with outliers  
# &nbsp;  
# 12. Dealing with missing numerical values  
# &nbsp;  
# 13. Check with ASSERT statement  
# &nbsp;  
# 14. Project Conclusion  
# 

# ## 1. Introduction to Python Cleaning

# Whenever we have to work with a real world dataset, the first problem that we face is to clean it. The real world dataset never comes clean. It consists lot of discrepancies in the dataset. So, we have to clean the dataset for further processing.
# 
# Cleaning data is the process of preparing the dataset for analysis. It is very important because the accuracy of machine learning, data mining models or even decision making are greatly affected because of poor quality of data.
# 
# So, data scientists spend a large amount of their time cleaning the dataset and transform them into a format with which they can work with. In fact, data scientists spend 80% of their time cleaning the data.
# 
# A very common scenario is that the dataset contains missing values coded as NaN. Also, the missing values are coded in different ways. The dataset may contain negative or invalid values. It may contain outliers. It may be in the untidy format. All of these are examples of a messy dataset.
# 
# In this project, I present several useful ways to handle these discrepancies in the dataset.

# ## 2. Python data cleaning - Prerequisites

# We need three Python libraries for the data cleansing process – NumPy, Pandas and Matplotlib.
# 
# • **NumPy** – NumPy is the fundamental Python library for scientific computing. It adds support for large and multi-dimensional arrays and matrices. It also supports large collection of high-level mathematical functions to operate on these arrays.
# 
# • **Pandas** - Pandas is a software library for Python programming language which provide tools for data manipulation and analysis tasks. It will enable us to manipulate numerical tables and time series using data structures and operations.
# 
# • **Matplotlib** - Matplotlib is the core data visualization library of Python programming language. It provides an object-oriented API for embedding plots into applications.

# ## 3. Import the required Python libraries

# We have seen that we need three Python libraries – NumPy, Pandas and Matplotlib for the data cleaning process. We need to import these libraries before we actually start using them.

# In[1]:


# importing the Python libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')


# ## 4. The source dataset

# For this project, I have created a fictious dataset. The dataset consists of details of 20 random people generated by ChatGPT.
# 
# The dataset imported from my local as follows:-

# In[2]:


data = "/Users/kailash/Desktop/Spend_Analysis.csv"

df = pd.read_csv(data)


# ## 5. Exploratory data analysis

# Now, it is time to understand the data. We should diagnose the data for any discrepancies by doing exploratory data analysis. As follows:-

# ### df.shape attribute

# We can check the dimensions of the data with df.shape attribute.

# In[3]:


df.shape


# we can see that our data have 20 rows and 10 columns.

# ### df.head() and df.tail() methods

# We can view the top five and bottom five rows of the dataset with **df.head()** and **df.tail()** methods respectively.

# In[4]:


df.head()


# In[5]:


df.tail()


# We can see that there are lot of discrepancies in the dataset.
# 
# For example, the age and sex columns are combined together with an underscore. There should be two separate columns of age and sex.
# 
# The height and weight columns contain missing values. Some values are coded as "xx", "?", "0" and negative values. They are all invalid values as height and weight must be positive real numbers.
# 
# The three columns spend_A, spend_B and spend_C denote spending at three supermarkets A,B and C. These columns must contain positive real numbers. The missing values in these columns denote nothing spend in that market. The negative value and the value coded as "xx" should be addressed properly.

# ### df.info() method

# We can get a concise summary of the dataset with df.info() method. This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.

# In[6]:


df.info()


# We can see that this method prints information of all columns. Several columns contain missing values. We have seen that columns contain missing and invalid values are coded differently. So, we need to explore this issue further.

# ### df.dtypes attribute

# We can check the data types of each column in the dataframe with df.dtypes attribute. The above command returns the data type of each column.

# In[7]:


df.dtypes


# Also, we can see that data types of height(cm) and weight(kg) columns are object data type. Again this is not true. The columns height(cm) and weight(kg) must contain positive real numbers. So, their data type must be float64.
# 
# Also, spend_A, spend_B and spend_C columns must contain numeric values. We can see that the data type of spend_A and spend_B columns are float64. But the data type of column spend_C is object. So, we need to convert its data type to float64.

# ### Handling invalid values

# There is an invalid value coded as "xx" in the height(cm) column. We can coerce invalid values to "NaN" using the errors keyword as follows:-

# In[8]:


df["Height(cm)"] = pd.to_numeric(df["Height(cm)"], errors='coerce')


# Similary, there is an invalid value coded as "xx" in the weight(kg) column. Again, we can coerce it using the errors keyword as follows:-

# In[9]:


df["Weight(kg)"] = pd.to_numeric(df["Weight(kg)"], errors='coerce')


# Similary, there is an invalid value coded as "xx" in the spend_C column. Again, we can coerce it using the errors keyword as follows:-

# In[10]:


df["Spend_C"] = pd.to_numeric(df["Spend_C"], errors='coerce')


# ### Check the data type again with df.dtypes

# In[11]:


df.dtypes


# Now, we can see that all the columns have appropriate data types. The columns height(cm) and weight(kg) have float64 data type. The columns spend_A, spend_B and spend_C have float64 data type.

# In[12]:


df.head()


# In[13]:


df.tail()


# We can see that all the invalid values(the values coded as "xx") and missing values are now coded as "NaN". The use of the keyword errors='coerce' enable us to convert all the invalid values into NaN.

# ### df.describe() method

# We can view the summary statistics of numerical columns with df.describe() method. It enable us to detect outliers in the data which require further investigation.

# In[14]:


df.describe()


# We can see that there are discrepancies in Height(cm) and Weight(kg) columns.
# 
# The minimum value of Height(cm) is 0. It is not possible because height cannot be 0.
# 
# The minimum and maximum values of Weight(kg) are -60 and 153. Weight cannot be negative and weight cannot be as high as 153. So, both are invalid values.
# 
# They are outliers and need to be properly addressed.

# ### df.columns attribute

# We can get the column labels of the dataframe with df.columns attribute.

# In[15]:


df.columns


# ## 6. Visual Exploratory Data Analysis

# Now, we should conduct data visualization to find discrepancies in the data. Data visualization is a great way to find errors in the data and detect outliers. They help us to detect patterns in the data.
# 
# We can use various types of plots for data visualization purpose. These plots are listed below:-
# 
# • **Bar plot**
# 
# • **Histograms**
# 
# • **Box plot**
# 
# • **Scatter plot**

# ### Bar plot

# A bar plot is a plot that presents data with rectangular bars with lengths proportional to the values that they represent.
# 
# We can plot a bar plot of the age column as follows:-

# In[16]:


df['Age'].plot(kind='bar')

plt.show()


# The bar plot shows that there are 20 counts of the age label. So, there are no missing values in the age column.

# ### Histograms

# We use histograms for plotting continuous data counts. A histogram is a representation of the distribution of data.
# 
# In this case, we use histograms for plotting distribution of data values of height(cm) and weight(kg) columns.
# 
# We can draw a histogram as follows:-

# In[17]:


df['Height(cm)'].plot(kind='hist')

plt.show()


# The above histogram shows that there is a data value between 0 and 25. When we take a closer look , we find that there is a value of 0.0 in the height(cm) column. It is not possible as height cannot be 0. So, we need to take care of that.

# In[18]:


df['Weight(kg)'].plot(kind='hist')

plt.show()


# Similar analysis of histogram of weight(kg) column shows that there is a negative value of -60 and a very high value of 153 in the weight(kg) column. Both are invalid values. Weight cannot be negative and also weight cannot be so high. So, we need to take care of these invalid values.

# ### Box plot

# We can visualize basic summary statistics with box plot. Box plot let us to detect outliers in the data. They help us to find minimum and maximum values. They present 25th, 50th, 75th percentiles. 50th percentile value is the median value.
# 
# We can draw a boxplot as follows:-

# In[19]:


df.boxplot(column='Height(cm)')

plt.show()


# I have drawn the boxplot of the height(cm) column in the above cell. It confirms that there is a value of 0.0 in the height(cm) column. It is not possible as height cannot be 0. So, we need to take care of that.

# In[20]:


df.boxplot(column='Weight(kg)')

plt.show()


# I have drawn the boxplot of the weight(kg) column in the above cell. The above boxplot confirms our findings that there is a negative value of -60 and a very high value of 153 in the weight(kg) column.
# 
# Both are invalid values. Weight cannot be negative and also weight cannot be so high. So, we need to take care of these invalid values.

# ### Scatter plot 

# Scatter plot help us to explore relationship between two numeric variables. it help us to identify potentially bad data.
# 
# We can draw a scatter plot of Height(cm) and Weight(kg) column as follows:-

# In[21]:


df.plot(kind='scatter',x='Height(cm)', y='Weight(kg)', c='DarkBlue')

plt.show()


# The above scatter plot does not depict anything due to lack of data values. We need more data to identify relationship or pattern between data values.

# ## 7. Findings of EDA and Visual EDA

# We can summarize the findings of EDA and visual EDA as follows:-
# 
# 1. The Dataset has 20 rows and 10 columns.<br>
# <br>
# 2. The Age and Sex columns are combined together with an underscore.<br>
# <br>
# 3. All the invalid values (coded as 'xx') and missing values in Height(cm), Weight(kg), Spend_A, Spend_B and Spend_c throughout the dataset are coded as NaN.<br>
# <br>
# 4. The data types of columns Height(cm), Weight(kg) and Spend_C are converted in Float64.<br>
# <br>
# 5. In Height(cm) column, there is value of 0.0. Which is not possible as height cannot be 0. We need to resolve it.<br>
# <br>
# 6. In Weight(kg) column, there is -60 and a very high value of 153 both are invalid, we need to resolve the issue.<br>
# <br>
# 7. The three columns Spend_A, Spend_B and Spend_C can be taken as three sections or super markets, the values must contain positive real numbers. The missing values in these columns denote nothing spent in that particular section. We need to handle these missing values properly.<br>
# <br>
# 8. In Spend_B column, there is a negative value -220. The amount spent cannot be negative. So, we need to take care of that as well.<br>

# ## 8. Split ‘age_sex’ column

# We should split the 'Age_Sex' column into two separate columns Age and Sex.
# 
# We can do this using the df.str.split() function as follows:-

# In[22]:


df[['Age','Sex']] = df.Age_Sex.str.split("_", expand = True)


# In[23]:


df.head()


# We can see that now we have two separate columns for Age and Sex.
# 
# Now, there is no need for the Age_Sex column. So, we should drop that column.
# 
# We can drop 'Age_Sex' column using the df.drop() method as follows:-

# In[24]:


df.drop(['Age_Sex'], axis=1, inplace=True)


# In[25]:


df


# We can see that the 'Age_Sex' column has been dropped from the dataframe.

# ## 9. Reorder the column lables

# We can reorder the columns for more pleasing visual appearance.
# 
# We can do it as follows:-

# In[26]:


df = df[['First_Name','Last_Name','Age','Sex','Section','Height(cm)','Weight(kg)','Spend_A','Spend_B','Spend_C']]


# In[27]:


df


# Now, we can move forward to deal with missing and negative numerical values.

# ## 10. Dealing with negative numerical values

# We have seen that, in the weight(kg) column, there is a negative value of -60.2. It is invalid value because weight cannot be negative. There is a high probability that weight is 60 kg and it is mistyped as -60. So, I will replace the negative value of -60.2 with positive value of 60.2.
# 
# We can do the same as follows:-

# In[28]:


# Suppress settingwithcopywarning

pd.set_option('mode.chained_assignment', None)


# In[29]:


df['Weight(kg)'].replace(-60.2, 60.2, inplace=True)


# In[30]:


df


# We can see that the negative value of -60.2 is replaced with positive value of 60.2.
# 
# Similarly, in the spend_B column, there is a negative value -220.10. The amount spent cannot be negative. So, we need to replace this negative value of -220.10 with positive value of 220.10.
# 
# We can do it as follows:-

# In[31]:


df['Spend_B'].replace(-220.10, 220.10, inplace=True)


# In[32]:


df


# Again, we can see that the negative value of -220.10 in Spend_B column is replaced with positive value of 220.10.

# ## 11. Dealing with outliers

# In the Height(cm) column, there is a value of 0.0. It is not possible as height cannot be 0. So, we need to resolve it.
# 
# I will replace the 0.0 value with the mean of the Height(cm) column. It can be done as follows:-

# In[33]:


mean = df['Height(cm)'].mean()


# In[34]:


df['Height(cm)'].replace(0.0, mean, inplace=True)


# In[35]:


df


# We can see that the data value of 0.0 in height(cm) column is replaced by a proper height value.
# 
# But, there are 6 characters after the decimal. To decrease the number of characters after the decimal point for all rows in a column, you can use the round() method in pandas. 
# 
# We can do the same as follows:-

# In[36]:


df['Height(cm)'] = df['Height(cm)'].round(2)


# In[37]:


df


# In the Weight(kg) column, there is a very high absurd value of 153.2. It is not possible to have so much weight. Hence, it is invalid value. There is a high chance that the weight is 53.2 kg and it is mistakenly typed as 153.2 kg. So, I will replace the 153.2 data value with 53.2.
# 
# It can be done as follows:-

# In[38]:


df['Weight(kg)'].replace(153.2, 53.2, inplace=True)


# In[39]:


df


# We can see that the data value of 153.2 in the weight(kg) column is replaced by 53.2.

# ## 12. Dealing with missing numerical values

# The following commands help us to deal with missing numerical values;
# 
# df.isnull( ):
# 
# This command checks whether each cell in a DataFrame contains a missing value (such as NaN or None). If the cell is missing a value, it returns True; otherwise, it returns False.
# 
# df.isnull( ).sum( ):
# 
# This command sums up the number of missing values in each column of the DataFrame. It returns the total count of missing values per column.
# 
# df.isna( ):
# 
# Similar to isnull( ), this command checks if each cell in a DataFrame is missing a value. It returns True for missing values and False for non-missing values.
# 
# df.notna( ):
# 
# This command is the opposite of isna( ). It checks if each cell contains a non-missing value. It returns True for cells with values and False for missing values.
# 
# df['col_name'].isna( ).sum( ):
# 
# This command counts how many missing values exist in a specific column (col_name) of the DataFrame. It returns the total number of missing values in that column.
# 
# So, we can check the number of missing values in each column in the dataset as follows:-

# In[40]:


df.isnull().sum()


# We can see that there are lots of missing values in the dataset.

# ### Fill missing values with a test statistic

# In this method, we fill the missing values with a test statistic like mean, median or mode of the particular feature the missing value belongs to. One can also specify a forward-fill or back-fill to propagate the next values backward or previous value forward.

# ### Filling missing values with a test statistic like mean

# mean = df [ 'col_name' ].mean( )
# 
# df [ 'col_name' ].fillna ( value = median, inplace = True )

# ### We can also use replace( ) in place of fillna( )

# df [ ‘col_name’ ].replace( to_replace = NaN, value = median, inplace = True )
# 
# If we choose this method, then we should compute the median value on the training set and use it to fill the missing values in the training set. Then we should save the median value that we have computed. Later, we will replace missing values in the test set with the median value to evaluate the system.
# 
# We can calculate the missing variables and use it to fill the missing values as follows:-

# In[41]:


mean_height = df['Height(cm)'].mean()

df['Height(cm)'].fillna(mean_height, inplace=True)


# In[42]:


mean_weight = df['Weight(kg)'].mean()

df['Weight(kg)'].fillna(mean_weight, inplace=True)


# In[43]:


mean_spend_A = df['Spend_A'].mean()

df['Spend_A'].fillna(mean_spend_A, inplace=True)


# In[44]:


mean_spend_B = df['Spend_B'].mean()

df['Spend_B'].fillna(mean_spend_B, inplace=True)


# In[45]:


mean_spend_C = df['Spend_C'].mean()

df['Spend_C'].fillna(mean_spend_C, inplace=True)


# We have calculated the mean variables and use them to fill the missing values.

# In[46]:


# Again checking for missing values

df.isnull().sum()


# We can see that there are no missing values in the dataframe.
# 
# We can confirm this with ASSERT statement as follwos:-

# ## 13. Check with ASSERT statement

# Finally, we can check for missing values. If we drop or fill missing values, we expect no missing values. We can write an assert statement to verify this. So, we can use an assert statement to programmatically check that no missing or unexpected ‘0’ value is present. This gives confidence that our code is running properly.
# 
# Assert statement will return nothing if the value being tested is true and will throw an AssertionError if the value is false.
# 
# Asserts
# 
# • assert 1 == 1 (return Nothing if the value is True)
# 
# • assert 1 == 2 (return AssertionError if the value is False)
# 
# We can check with assert statement as follows:-

# In[47]:


#assert that there are no missing values in the dataframe

assert pd.notnull(df).all().all()


# In[48]:


#assert all values are greater than 0

assert (df.select_dtypes(include=['number']) > 0).all().all()


# The assert statement returns nothing. So, we can conclude that there are no missing values in the dataset and all the values are greater than zero.
# 
# We can confirm this by looking at the dataframe.

# In[49]:


df


# We can see that there are no missing or negative values in the dataframe.
# 
# But, there are 6 characters after the decimal. To decrease the number of characters after the decimal point, we can use the round() method in pandas as done above for the Height(cm).
# 
# We can do the same as follows:-

# In[50]:


# Rounding columns to 2 decimal places

df[['Height(cm)', 'Weight(kg)', 'Spend_A', 'Spend_B', 'Spend_C']] = df[['Height(cm)', 'Weight(kg)', 'Spend_A', 'Spend_B', 'Spend_C']].round(2)


# In[51]:


df


# The dataset is now clean, organized, and free of errors. It’s ready to be used for analysis, visualization, or modeling in the next steps.

# ## 14. Project Conclusion

# As shown in the project, through systematic techniques like data inspection, transformation, and validation, this project demonstrates how to convert messy, incomplete, or inconsistent data into a structured format ready for analysis. These steps not only improve the quality of the data but also save significant time and effort in downstream processes like visualization, statistical modeling, and machine learning.
# 
# Ultimately, effective data cleaning enhances the accuracy of insights and the reliability of decisions drawn from data. Whether addressing business challenges, conducting research, or developing predictive models, clean and well-prepared data forms the foundation for success. By mastering these techniques, analysts and data scientists can confidently tackle real-world data problems and unlock the full potential of their datasets.
